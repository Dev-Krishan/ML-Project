# -*- coding: utf-8 -*-
"""Heart_disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18DB_Tt7ntKkDTK_cnbioBE2YBMjKScJE

#Predicting Heart Disease using Machine Learning
This notebook will introduce some foundation machine learning and data science concepts by exploring the problem of heart disease classification.

### Some steps we need to do
* Exploratory data analysis (EDA) - the process of going through a dataset and finding out more about it.
* Model training - create model(s) to learn to predict a target variable based on other variables.
* Model evaluation - evaluating a models predictions using problem-specific evaluation metrics.
* Model comparison - comparing several different models to find the best one.
* Model fine-tuning - once we've found a good model, how can we improve it?
* Feature importance - since we're predicting the presence of heart disease, are there some things which are more important for prediction?
* Cross-validation - if we do build a good model, can we be sure it will work on unseen data?

##**1. Problem definition**
In our case, the problem we will be exploring is binary classification (a sample can only be one of two things).

This is because we're going to be using a number of differnet features (pieces of information) about a person to predict whether they have heart disease or not.

##**2. Data**
The original data came from the Cleveland database from UCI Machine Learning Repository.

Howevever, we've downloaded it in a formatted way from Kaggle.

The original database contains 76 attributes, but here only 14 attributes will be used. Attributes (also called features) are the variables what we'll use to predict our target variable.

##**3, Features**
* age - age in years
* sex - (1 = male; 0 = female)
* cp - chest pain type
  * 0: Typical angina: chest pain related decrease blood supply to the heart
  * 1: Atypical angina: chest pain not related to heart
  * 2: Non-anginal pain: typically esophageal spasms (non heart related)
  * 3: Asymptomatic: chest pain not showing signs of disease
* trestbps - resting blood pressure (in mm Hg on admission to the hospital)
anything above 130-140 is typically cause for concern
* chol - serum cholestoral in mg/dl
* serum = LDL + HDL + .2 * triglycerides
above 200 is cause for concern
* fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)
'>126' mg/dL signals diabetes
restecg - resting electrocardiographic results
  * 0: Nothing to note
  * 1: ST-T Wave abnormality
can range from mild symptoms to severe problems
signals non-normal heart beat
  * 2: Possible or definite left ventricular hypertrophy Enlarged heart's main pumping chamber
* thalach - maximum heart rate achieved
* exang - exercise induced angina (1 = yes; 0 = no)
* oldpeak - ST depression induced by exercise relative to rest
looks at stress of heart during excercise
unhealthy heart will stress more
* slope - the slope of the peak exercise ST segment
  * 0: Upsloping: better heart rate with excercise (uncommon)
  * 1: Flatsloping: minimal change (typical healthy heart)
  * 2: Downslopins: signs of unhealthy heart
* ca - number of major vessels (0-3) colored by flourosopy
colored vessel means the doctor can see the blood passing through
the more blood movement the better (no clots)
thal - thalium stress result
  * 1,3: normal
  * 6: fixed defect: used to be defect but ok now
  * 7: reversable defect: no proper blood movement when excercising
* target - have disease or not (1=yes, 0=no) (= the predicted attribute
"""

# Commented out IPython magic to ensure Python compatibility.
#Libraries for EDA and data visulaization.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

#Models for Scikit learn
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

#Model Evaluation.
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score,f1_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import plot_roc_curve
from sklearn.model_selection import cross_val_score

#loading data
df = pd.read_csv("heart-disease.csv")
print(df.head())
print(df.shape)

"""##Questions.
* Question trying to solve
* what kind of data we have and do we we treat different type
* What's the missing data and how are we dealing with it.
* What are outliers and why should we care about them.
* how can we add, remove, transform features to get more from our data.
"""

#Lets find out how many class of each there are (! for have heart disease and 0 for no heart disease.)
df.target.value_counts()

df["target"].value_counts().plot(kind ="bar", color =["red","orange"])

#Getting some info from our dataframe.
df.info()

#CHeck any null value.
df.isnull().sum()

df.describe()

"""##Finding patterns."""

#heart disease frequency according to sex 1-male  0-female
df["sex"].value_counts()

#comapre target column with sex
pd.crosstab(df.target, df.sex)

#create a visual of cross tab.
pd.crosstab(df.target, df.sex).plot(kind="bar", figsize=(10,6))
plt.xlabel(" 0-No Disease 1_Disease")
plt.legend(["FeMale", "male"])
plt.xticks(rotation= 0)

#using two independent variables with target and finding pattern.
plt.figure(figsize=(10,6))
#people with disease heart rate
plt.scatter(df["age"][df.target==1],
            df["thalach"][df.target == 1])
#No disease heart rate
plt.scatter(df["age"][df.target==0],
            df["thalach"][df.target == 0])
plt.legend(["Disease", "No disease"])
plt.xlabel("age")
plt.ylabel("Maxm heart rate")

#plotting histogram for age
df.age.plot(kind="hist")

##3heart disease frequency per chest pain type
#0: Typical angina: chest pain related decrease blood supply to the heart
#1: Atypical angina: chest pain not related to heart
#2: Non-anginal pain: typically esophageal spasms (non heart related)
#3: Asymptomatic: chest pain not showing signs of disease
pd.crosstab(df.cp,df.target)

#make the chest pain cross tab viusal
pd.crosstab(df.cp,df.target).plot(kind = "bar")
plt.xticks(rotation = 0)
plt.legend(["NO disease", "Disease"])
plt.xlabel("Chest Pain category")
plt.ylabel("Amount")
plt.title("Heart disease vs chest pain categories")

##Building correlation.
corr = df.corr()
corr

fig,ax = plt.subplots(figsize = (10,6))
ax = sns.heatmap(corr,
                 linewidths = 0.5,
                 annot = True,
                 fmt = ".2f",
                 cmap = "YlGnBu")

#Data Exploaration (EDA)
sns.pairplot(df)

#preparing our data for ML.
np.random.seed(42)
X = df.drop("target", axis =1)
Y = df["target"]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state =2)

print(X_train.shape)
print(X_test.shape)
print(X.shape)
print(Y.shape)
print(Y_train.shape)
print(Y_test.shape)

#PUt models in dictionary.
models = {"Logistic Regression" : LogisticRegression(),
          "KNN" : KNeighborsClassifier(),
          "Random Forest Classifier": RandomForestClassifier()
          }

#Create a  func to fit and score models.
def fit_and_score(models, X_train, X_test, Y_train, Y_test):
  '''
  Fit and evaluate the machine learning models.
  models : a dict of different ML classifier models.
  X_train : Training data(no labels)
  X_test  : Testing data(no labels)
  Y_train : Training labels
  Y_test  : Test labels 
  '''
  #set random seed
  np.random.seed(42)

  #make a dict to kep models score
  model_scores = {}

  #loop through models.
  for name, model in models.items():
    #fit the data
    model.fit(X_train, Y_train)
    #Evaluate the model and append in score dict
    model_scores[name] = model.score(X_test, Y_test)
  
  return model_scores

model_scores = fit_and_score(models = models,
                              X_train = X_train,
                              X_test = X_test,
                              Y_train = Y_train , 
                              Y_test = Y_test)
model_scores

#Model_comparison
model_compare = pd.DataFrame(model_scores, index = ["Accuracy"])
model_compare.T.plot.bar()

"""**Tuning Hyperparameters**Lets tune knn by hand"""

train_scores = []
test_scores = []

#Create a list of different value of neighbors
neighbors = range(1,21)
knn = KNeighborsClassifier()

#Loop through n different neighbors
for i in neighbors:
  knn.set_params(n_neighbors = i)

  #fit model
  knn.fit(X_train, Y_train)

  #Update training scores
  train_scores.append(knn.score(X_train, Y_train))

  #update test scores list
  test_scores.append(knn.score(X_test, Y_test))

train_scores

test_scores

plt.plot(neighbors, train_scores, label = "Train score")
plt.plot(neighbors, test_scores, label =" test score")
plt.xticks(np.arange(1,22))
plt.xlabel("No of k Neighbors")
plt.ylabel("Model score")
plt.legend()
print(f"Maxm score is {max(test_scores)*100:.2f}%")

"""#HYpermeter tuning by RandoizedSearchCV
We will tune.
* LOgistic Regression
* random forest 
by this technique
"""

#Create a grid for Logistic Regression
log_reg_grid = {"C": np.logspace(-4,4,20),
                "solver": ["liblinear"]
                }

#crete a grid for random forest classifier
rf_grid = {"n_estimators": np.arange(10,1000,50),
           "max_depth": [None, 3, 5, 10],
           "min_samples_split":np.arange(2,20,2),
           "min_samples_leaf":np.arange(1,10,2)
}

#Tune Logistic Regression
np.random.seed(42)
rs_log_reg = RandomizedSearchCV(LogisticRegression(),
                                param_distributions = log_reg_grid,
                                cv =5,
                                n_iter =20,
                                verbose = True)

#Fit random  hypermeter model for logistic
rs_log_reg.fit(X_train, Y_train)

rs_log_reg.best_params_

rs_log_reg.score(X_test,Y_test)

#lets do for random forest
np.random.seed(42)
rs_rf = RandomizedSearchCV(RandomForestClassifier(),
                                param_distributions = rf_grid,
                                cv =5,
                                n_iter =20,
                                verbose = True)
rs_rf.fit(X_train, Y_train)

rs_rf.best_params_

rs_rf.score(X_test, Y_test)

"""###grid search hyperparameter tuning"""

np.random.seed(42)
log_reg_grid = {"C": np.logspace(-4,4,20),
                "solver": ["liblinear"]
                }

gs_log_reg = GridSearchCV(LogisticRegression(),
                                param_grid = log_reg_grid,
                                cv =5,
                                verbose = True)

#Fit random  hypermeter model for logistic
gs_log_reg.fit(X_train, Y_train)

gs_log_reg.best_params_

gs_log_reg.score(X_test, Y_test)

# Grid search for random forest
rf_grid = {"n_estimators": np.arange(10,100,20),
           "max_depth": [None, 3, 5, 10],
           "min_samples_split":np.arange(2,10,2),
           "min_samples_leaf":np.arange(1,10,2)
}
np.random.seed(42)
gs_rf = GridSearchCV(RandomForestClassifier(),
                                param_grid = rf_grid,
                                cv =5,
                                
                                verbose = True)
gs_rf.fit(X_train, Y_train)

gs_rf.score(X_test, Y_test)

gs_rf.best_params_

y_pred = gs_log_reg.predict(X_test)

Y_test

#Plot Roc curve
plot_roc_curve(gs_log_reg, X_test, Y_test)

#confusion matrix
print(confusion_matrix(Y_test, y_pred))

sns.set(font_scale = 1.5)

def plot_conf_mat(Y_test, y_pred):
  """
  Plots nice looking heatmap
  """
  fig, ax = plt.subplots(figsize = (3,3))
  ax = sns.heatmap(confusion_matrix(Y_test, y_pred),
                   annot = True,
                   cbar = False)
  plt.xlabel("Predcited label")
  plt.ylabel("True label")


plot_conf_mat(Y_test, y_pred)

#Classification report ads well as cross val score
print(classification_report(Y_test, y_pred))

#create a new classifier with best parameter
clf = LogisticRegression(C= 0.23357214690901212,
                         solver = "liblinear")

#Cross val accuracy
cv_acc = cross_val_score(clf,
                         X,
                         Y,
                         cv = 5,
                         scoring = "accuracy")
cv_acc

#cross val precison
cv_precision = cross_val_score(clf,
                         X,
                         Y,
                         cv = 5,
                         scoring = "precision")
np.mean(cv_precision)

#cross val recall
cv_recall = cross_val_score(clf,
                         X,
                         Y,
                         cv = 5,
                         scoring = "recall")
np.mean(cv_recall)

#cross val f1 score
cv_f1 = cross_val_score(clf,
                         X,
                         Y,
                         cv = 5,
                         scoring = "f1")
cv_f1

clf = LogisticRegression(C= 0.23357214690901212,
                         solver = "liblinear")
clf.fit(X_train, Y_train)

#feature importance
feature_dict = dict(zip(df.columns, list(clf.coef_[0])))
feature_dict

feature_df = pd.DataFrame(feature_dict, index = [0])
feature_df.T.plot.bar(title ="Feature Importance", legend = False)

"""The larger the value (bigger bar), the more the feature contributes to the models decision.

If the value is negative, it means there's a negative correlation. And vice versa for positive values.

For example, the sex attribute has a negative value of -0.904, which means as the value for sex increases, the target value decreases.
"""

